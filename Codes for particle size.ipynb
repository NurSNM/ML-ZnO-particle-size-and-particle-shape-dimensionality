{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "333de7bd",
   "metadata": {},
   "source": [
    "# Prediction of particle size of ZnO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0418b735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as numpy\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error, mean_absolute_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.lines import Line2D\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab32731b",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b855e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"dataset-pathway\"\n",
    "data = pd.read_csv(file_path)\n",
    "print(data.head())\n",
    "\n",
    "data.isnull().sum()\n",
    "\n",
    "data = data.dropna(subset=['Max. size (nm)']) # remove rows with missing data for Max.size\n",
    "print(f'New rows and columns', data.shape) # check for the new number of rows and columns\n",
    "data.isnull().sum() # check if all missing values have been removed from 'Max. size'\n",
    "\n",
    "## Encode categorical features using OHE\n",
    "\n",
    "ohe = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "encoded_features = ohe.fit_transform(data[['Method category', 'Solvent type']])\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=ohe.get_feature_names_out(['Method category', 'Solvent type']))\n",
    "data = pd.concat([data.reset_index(drop=True), encoded_df.reset_index(drop=True)], axis=1) # concatenate using reset_index(drop=True)\n",
    "\n",
    "encoded_columns = ['Method category', 'Solvent type'] # Drop original categorical columns\n",
    "data = data.drop(columns=encoded_columns)\n",
    "print(data)\n",
    "\n",
    "## Impute missing data\n",
    "\n",
    "exclude_column = ['Max. size (nm)']\n",
    "columns_to_impute = [col for col in data.columns if col not in exclude_column]\n",
    "categorical_features = data[columns_to_impute].select_dtypes(include=['object']).columns\n",
    "numerical_features = data[columns_to_impute].select_dtypes(include=['number']).columns\n",
    "\n",
    "imputer = IterativeImputer(max_iter=10, random_state=0, verbose=2)\n",
    "ohe_columns = encoded_df.columns  # Columns created by OneHotEncoder\n",
    "data[ohe_columns] = imputer.fit_transform(data[ohe_columns])\n",
    "\n",
    "data[numerical_features] = imputer.fit_transform(data[numerical_features])\n",
    "\n",
    "# combine the imputed numerical and categorical data\n",
    "\n",
    "imputed_data = pd.concat([data[ohe_columns], data[numerical_features], data[['Max. size (nm)']]], axis=1)\n",
    "imputed_data.isnull().sum()\n",
    "\n",
    "# Feature scaling\n",
    "\n",
    "numerical_features = data.select_dtypes(include='number').drop(columns=['Max. size (nm)'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(numerical_features)\n",
    "\n",
    "data_scaled = data.copy()\n",
    "data_scaled[numerical_features.columns] = scaled_features\n",
    "data = data_scaled\n",
    "final_data = data\n",
    "print(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef31a8c1",
   "metadata": {},
   "source": [
    "# Train model - 30 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e063645",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Store all results\n",
    "results = []\n",
    "best_rmse = np.inf\n",
    "\n",
    "# Run model 30 times\n",
    "for seed in range(30):\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "    # Train model\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict both train and test\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    train_rmse = root_mean_squared_error(y_train, y_train_pred)\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    test_rmse = root_mean_squared_error(y_test, y_test_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "    # Store metrics\n",
    "    results.append({\n",
    "        'seed': seed,\n",
    "        'Train_R2': train_r2, 'Train_RMSE': train_rmse, 'Train_MAE': train_mae,\n",
    "        'Test_R2': test_r2, 'Test_RMSE': test_rmse, 'Test_MAE': test_mae\n",
    "    })\n",
    "\n",
    "    # Track best model based on test RMSE\n",
    "    if test_rmse < best_rmse:\n",
    "        best_rmse = test_rmse\n",
    "        best_seed = seed\n",
    "        best_model = model\n",
    "        best_y_train = y_train.copy()\n",
    "        best_y_train_pred = y_train_pred.copy()\n",
    "        best_y_test = y_test.copy()\n",
    "        best_y_test_pred = y_test_pred.copy()\n",
    "\n",
    "    print(f\"Seed {seed} - Train RMSE: {train_rmse:.3f}, Test RMSE: {test_rmse:.3f}\")\n",
    "\n",
    "# Save results\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv(\"RF_30seeds_metrics_train_test.csv\", index=False)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nSummary of metrics over 30 runs:\")\n",
    "print(df_results.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92874732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "train_residuals = best_y_train - best_y_train_pred\n",
    "test_residuals = best_y_test - best_y_test_pred\n",
    "\n",
    "# Combine for one plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot train residuals\n",
    "plt.scatter(best_y_train_pred, train_residuals, color='blue', alpha=0.5, edgecolor='k', label='Train')\n",
    "\n",
    "# Plot test residuals\n",
    "plt.scatter(best_y_test_pred, test_residuals, color='green', alpha=0.6, edgecolor='k', label='Test')\n",
    "\n",
    "# Horizontal zero line\n",
    "plt.axhline(0, color='red', linestyle='--', linewidth=2, label='Zero error')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Predicted Values', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Residuals', fontsize=14, fontweight='bold')\n",
    "plt.title(f'Residuals Plot', fontsize=14, fontweight='bold')\n",
    "plt.legend(title_fontsize=13, fontsize=14, loc='best', frameon=True)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6901da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction error plot\n",
    "\n",
    "# Combine train and test into one for cleaner plot (optional)\n",
    "y_all_actual = np.concatenate([y_train, y_test])\n",
    "y_all_pred = np.concatenate([y_train_pred, y_pred])\n",
    "labels = ['Train'] * len(y_train) + ['Test'] * len(y_test)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Scatterplot with regression line\n",
    "sns.scatterplot(x=y_all_pred, y=y_all_actual, hue=labels, alpha=0.6, edgecolor='k')\n",
    "sns.regplot(x=y_all_pred, y=y_all_actual, scatter=False, color='black', line_kws={\"label\": \"Best Fit Line\"})\n",
    "\n",
    "# Identity line (ideal prediction)\n",
    "min_val = min(y_all_actual.min(), y_all_pred.min())\n",
    "max_val = max(y_all_actual.max(), y_all_pred.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='Ideal (y = x)')\n",
    "\n",
    "# Labels and legend\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Actual Values')\n",
    "plt.title('Prediction Error Plot with Identity and Best Fit Lines')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c84c874",
   "metadata": {},
   "source": [
    "# SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4694145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SHAP explainer object\n",
    "explainer = shap.Explainer(model, X_train)\n",
    "\n",
    "# Calculate SHAP values\n",
    "#shap_values = explainer(X_test)  # Use test data to explain predictions\n",
    "shap_values = explainer(X_test, check_additivity=False)\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "\n",
    "# Convert the SHAP values to a Pandas DataFrame for easy manipulation\n",
    "shap_df = pd.DataFrame(shap_values.values, columns=X.columns)\n",
    "\n",
    "# Calculate the mean absolute SHAP value for each feature\n",
    "mean_abs_shap = shap_df.abs().mean(axis=0)\n",
    "\n",
    "# Sort the features based on their mean absolute SHAP value\n",
    "top_10_features = mean_abs_shap.sort_values(ascending=False).head(10)\n",
    "\n",
    "# Print the top 10 features\n",
    "print(top_10_features)\n",
    "\n",
    "# Plot the top 10 features using a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_10_features.plot(kind='bar', color='skyblue')\n",
    "#plt.title('Top 10 Features Based on Mean Absolute SHAP Values')\n",
    "plt.xlabel('Features', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Mean Absolute SHAP Value', fontsize=12, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
