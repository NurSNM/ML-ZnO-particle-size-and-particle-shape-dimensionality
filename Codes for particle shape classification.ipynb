{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8280ebb4",
   "metadata": {},
   "source": [
    "# Particle shape dimensionality prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f618c4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361b1582",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e322cfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"dataset-pathway\"\n",
    "data = pd.read_csv(file_path)\n",
    "print(data.head())\n",
    "\n",
    "data.isnull().sum()\n",
    "\n",
    "# drop rows\n",
    "data = data[data['Dimensionality'].notna()]\n",
    "\n",
    "# Check the new number of rows and columns\n",
    "print(data.shape)\n",
    "\n",
    "## Encode categorical features using OHE\n",
    "\n",
    "ohe = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "encoded_features = ohe.fit_transform(data[['Method category', 'Solvent type']])\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=ohe.get_feature_names_out(['Method category', 'Solvent type']))\n",
    "data = pd.concat([data.reset_index(drop=True), encoded_df.reset_index(drop=True)], axis=1) # concatenate using reset_index(drop=True)\n",
    "\n",
    "encoded_columns = ['Method category', 'Solvent type'] # Drop original categorical columns\n",
    "data = data.drop(columns=encoded_columns)\n",
    "print(data)\n",
    "\n",
    "## Impute missing data\n",
    "\n",
    "exclude_column = ['Dimensionality']\n",
    "columns_to_impute = [col for col in data.columns if col not in exclude_column]\n",
    "categorical_features = data[columns_to_impute].select_dtypes(include=['object']).columns\n",
    "numerical_features = data[columns_to_impute].select_dtypes(include=['number']).columns\n",
    "\n",
    "imputer = IterativeImputer(max_iter=10, random_state=0, verbose=2)\n",
    "ohe_columns = encoded_df.columns  # Columns created by OneHotEncoder\n",
    "data[ohe_columns] = imputer.fit_transform(data[ohe_columns])\n",
    "\n",
    "data[numerical_features] = imputer.fit_transform(data[numerical_features])\n",
    "\n",
    "# combine the imputed numerical and categorical data\n",
    "\n",
    "imputed_data = pd.concat([data[ohe_columns], data[numerical_features], data[['Dimensionality']]], axis=1)\n",
    "imputed_data.isnull().sum()\n",
    "\n",
    "# Feature scaling\n",
    "numerical_features = imputed_data.select_dtypes(include='number')\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(numerical_features)\n",
    "\n",
    "data_scaled = imputed_data.copy()\n",
    "data_scaled[numerical_features.columns] = scaled_features\n",
    "imputed_data = data_scaled\n",
    "final_data = imputed_data\n",
    "print(final_data)\n",
    "\n",
    "# Display the result\n",
    "print(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae07ee12",
   "metadata": {},
   "source": [
    "# Train model - 30 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47aff0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data set\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Loop over 30 seeds\n",
    "for seed in range(30):\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "    # Train model\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=seed)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_train_pred = model.predict(X_train)\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_train, y_train_pred)\n",
    "    prec = precision_score(y_train, y_train_pred, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_train, y_train_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_train, y_train_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    # Store results\n",
    "    results.append({\n",
    "        'seed': seed,\n",
    "        'Accuracy': acc,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'F1-score': f1\n",
    "    })\n",
    "\n",
    "    print(f\"Seed {seed} - Acc: {acc:.3f}, Prec: {prec:.3f}, Rec: {rec:.3f}, F1: {f1:.3f}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nSummary of classification metrics over 30 runs:\")\n",
    "print(df_results.describe())\n",
    "\n",
    "# Save to CSV\n",
    "df_results.to_csv(\"RFClassifier_30seeds_metrics_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2a34f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data set\n",
    "X = final_data.drop(columns=['Dimensionality'])\n",
    "y = final_data['Dimensionality']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Loop over 30 seeds\n",
    "for seed in range(30):\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "    # Train model\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=seed)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    # Store results\n",
    "    results.append({\n",
    "        'seed': seed,\n",
    "        'Accuracy': acc,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'F1-score': f1\n",
    "    })\n",
    "\n",
    "    print(f\"Seed {seed} - Acc: {acc:.3f}, Prec: {prec:.3f}, Rec: {rec:.3f}, F1: {f1:.3f}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nSummary of classification metrics over 30 runs:\")\n",
    "print(df_results.describe())\n",
    "\n",
    "# Save to CSV\n",
    "df_results.to_csv(\"RFClassifier_30seeds_metrics_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de673c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Reds\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b496699b",
   "metadata": {},
   "source": [
    "# 10-k fold validation by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa8cec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report by class for training set (10-k fold)\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Cross-validation scores\n",
    "scores = cross_val_score(rf, X_train, y_train, cv=10, scoring='accuracy')\n",
    "print('Cross-Validation scores:', scores)\n",
    "print('Mean Accuracy:', scores.mean())\n",
    "\n",
    "# Cross-validated predictions\n",
    "y_pred = cross_val_predict(rf, X_train, y_train, cv=10)\n",
    "\n",
    "# Classification report as dictionary\n",
    "report = classification_report(y_train, y_pred, output_dict=True)\n",
    "\n",
    "# Print per-class metrics\n",
    "for label, metrics in report.items():\n",
    "    if label not in [\"accuracy\", \"macro avg\", \"weighted avg\"]:\n",
    "        print(f\"\\nClass: {label}\")\n",
    "        print(f\"  Precision: {metrics['precision']:.2f}\")\n",
    "        print(f\"  Recall:    {metrics['recall']:.2f}\")\n",
    "        print(f\"  F1-score:  {metrics['f1-score']:.2f}\")\n",
    "        print(f\"  Support:   {metrics['support']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2cac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report by class for testing set (10-k fold)\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Cross-validation scores\n",
    "scores = cross_val_score(rf, X_test, y_test, cv=10, scoring='accuracy')\n",
    "print('Cross-Validation scores:', scores)\n",
    "print('Mean Accuracy:', scores.mean())\n",
    "\n",
    "# Cross-validated predictions\n",
    "y_pred = cross_val_predict(rf, X_test, y_test, cv=10)\n",
    "\n",
    "# Classification report as dictionary\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "# Print per-class metrics\n",
    "for label, metrics in report.items():\n",
    "    if label not in [\"accuracy\", \"macro avg\", \"weighted avg\"]:\n",
    "        print(f\"\\nClass: {label}\")\n",
    "        print(f\"  Precision: {metrics['precision']:.2f}\")\n",
    "        print(f\"  Recall:    {metrics['recall']:.2f}\")\n",
    "        print(f\"  F1-score:  {metrics['f1-score']:.2f}\")\n",
    "        print(f\"  Support:   {metrics['support']}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
